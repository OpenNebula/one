{#
name: On-Prem NFS Cluster
description: |
  Creates a cluster using your on-prem (local) infrastructure.
  * Requirements. Hosts must have the OS installed and be reachable via SSH. An NFS export must exist and be reachable from all hosts. The shared export MUST be owned by oneadmin (UID/GID 9869). And mounted in the the front-end in the designeted NFS mount pat in the designeted NFS mount path.
  * Storage. VM images and system files are stored on shared NFS storage.
  * Network. VMs use a flat bridged network via a physical NIC on every host.

fireedge:
  logo: "onprem_ssh_cluster.png"

user_inputs:
  - name: phydev_name
    description: Host NIC used to bridge the VM network (must exist on all hosts), e.g. eth0 or ens3
    type: string
    default: "eth0"

  - name: network_address
    description: Network base address for the bridged VM network, e.g. 10.0.0.0
    type: string
    default: "10.0.0.0"

  - name: network_mask
    description: Network mask for the bridged VM network, e.g. 255.255.255.0
    type: string
    default: "255.255.255.0"

  - name: gateway
    description: Default gateway IP for VMs in this network
    type: string
    default: "10.0.0.1"

  - name: dns
    description: DNS server IP used by VMs
    type: string
    default: "8.8.8.8"

  - name: ip
    description: First IP address to allocate for VMs from this network
    type: string
    default: "10.0.0.50"

  - name: size
    description: Number of consecutive IPs to allocate starting from the first IP
    type: number
    default: 100

  - name: nfs_server
    description: NFS server IP or hostname
    type: string
    default: "172.20.0.1"

  - name: nfs_export
    description: NFS exported path for this cluster, e.g. /srv/one_cluster_prod. The path MUST be owned by UID and GID 9869
    type: string
    default: "/srv/one_cluster_prod"

  - name: nfs_mount_path
    description: Mount point on nodes for the NFS export, e.g. /mnt/opennebula/cluster_prod
    type: string
    default: "/mnt/opennebula/cluster_prod"
#}

---
all:
  vars:
    ansible_user: ubuntu
    one_version: "{{ one.version }}"

    vn:
      private_onprem_network:
        managed: true
        template:
          vn_mad: bridge
          phydev: "{{ user_inputs.phydev_name }}"
          network_address: "{{ user_inputs.network_address }}"
          network_mask: "{{ user_inputs.network_mask }}"
          gateway: "{{ user_inputs.gateway }}"
          dns: "{{ user_inputs.dns }}"
          ar:
            - type: IP4
              ip: "{{ user_inputs.ip }}"
              size: "{{ user_inputs.size }}"

    ds:
      mode: generic
      config:
        SYSTEM_DS:
          onprem_system_ds:
            id: "{{ one.system_ds_id }}"
            symlink:
              src: "{{ user_inputs.nfs_mount_path }}/{{ one.system_ds_id }}"
              groups:
                - frontend
                - node
            template:
              type: SYSTEM_DS
              tm_mad: shared

        IMAGE_DS:
          onprem_image_ds:
            id: "{{ one.image_ds_id }}"
            symlink:
              src: "{{ user_inputs.nfs_mount_path }}/{{ one.image_ds_id }}"
              groups:
                - frontend
                - node
            template:
              type: IMAGE_DS
              ds_mad: fs
              tm_mad: shared
              safe_dirs: /var/tmp /tmp

    # Mount the NFS export on all nodes (front-end and hosts)
    fstab:
      - src: "{{ user_inputs.nfs_server }}:{{ user_inputs.nfs_export }}"
        path: "{{ user_inputs.nfs_mount_path }}"
        fstype: nfs
        opts: rw,soft,intr,async,rsize=32768,wsize=32768

frontend:
  hosts:
    f1: { ansible_user: root, ansible_host: "{{ one.frontend_ip }}" }

node:
  hosts:
    {% for node in one.nodes %}
    n{{ loop.index }}: { ansible_user: root, ansible_host: "{{ node }}" }
    {% endfor %}
